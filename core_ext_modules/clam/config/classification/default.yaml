# hydra configuration
hydra:
  searchpath:
    - file:///local/data1/iulta54/Code/BTB_DEEP_LEARNING/configs

# Default configuration file for running MIL training using CLAM framework (not CLAM per-se)
defaults:
- _self_
- task : BTB_tumor_category
- local_directories : local_directories

# ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤ experiment information
feature_extractor : 'vit_uni' # Specify which feature extractor was used to obtain obtain the patch encodings. This is used to get build the experiment name and select the right features from the feature directory.
magnification : 20
patch_size : 224

# ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤ experiment setting
exp_code :  # name of the experiment for saving the results
results_dir : /local/data1/iulta54/Code/BTB_DEEP_LEARNING/outputs # Path to where the results should be saved

label_frac : 1.0 # Fraction of given training images to use [0, 1]
seed : 29122009 # For reproducibility
log_data : True # Set to True to save the logs to TensorBoard
testing : False # Debugging tool (TODO check the code and see what it does)


# ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤ cross validation settings
k : 5 # Number of folds to run 
k_start : -1 # Sets the number of the fold from where it starts training (-1 means the last one)
k_end : -1 # Sets the number of the fold from where it ends training (-1 means the first one)

# ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤ optimization settings
lr : 0.0001 # Learning rate
lr_scheduler : 'cosine'
reg : 0.001 # Weight decay
opt : adamW # Select between adam or sgd optimizer
bag_loss : ce # Select between ce (categorical cross-entropy) or svm (support vector machine) loss for the slide level classification.
use_class_weights : false # if to use a weighted bag loss with weights computed on the training set ()
bag_weight : 0.7 # Weight coefficient for bag-level loss

# ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤ model settings
encoding_size : null # Specify the size of each feature vector. This is not needed anymore since the encoding sice is infered from the .h5 files.
model_type : abmil # Select the type of classification model to use. Chose between clam_sb (single branch), clam_mb (multi branch), mil
model_size : null # Size of the model. Chose between small and big. Does not affect the model_type==mil
drop_out : 0.25 # Specify the drop-out rate [0, 1]

# ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤ CLAM specific settings
no_inst_cluster : False # Disable the instance-level clustering
inst_loss : svm # Instance level clustering loss. Chose between svm, ce, null
B : 8 # Number of positive/negative patches to sample for clam

# ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤ training settings
min_epochs: 10 # Number of epochs that the model will train before using Early stopping applies
max_epochs : 12 # Max number of epochs
early_stopping : True # Set to True if early stopping should be set
patience : 5 
weighted_sample : True # Set to True if weighted sampling should be performed

# ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤ dataset (not task) settings
patient_strat : False
shuffle : False # TODO - check code to see what this does
print_info : False
skip_missing_slide_ids : False

